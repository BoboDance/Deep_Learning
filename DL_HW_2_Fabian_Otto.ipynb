{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "np.random.seed(12345)\n",
    "\n",
    "\n",
    "def initialize(input_dim, hidden_dim, output_dim, batchsize):\n",
    "    W1 = np.random.randn(hidden_dim, input_dim) * 0.01\n",
    "    b1 = np.zeros((hidden_dim,))\n",
    "    W2 = np.random.randn(hidden_dim, hidden_dim) * 0.01\n",
    "    b2 = np.zeros((hidden_dim,))\n",
    "    W3 = np.random.randn(output_dim, hidden_dim) * 0.01\n",
    "    b3 = np.zeros((output_dim,))\n",
    "\n",
    "    parameters = [W1, b1, W2, b2, W3, b3]\n",
    "    x = np.random.rand(input_dim, batchsize)\n",
    "    y = np.random.randn(output_dim, batchsize)\n",
    "    return parameters, x, y\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return np.divide(1.0, np.add(1.0, np.exp(-x)))\n",
    "\n",
    "\n",
    "def dsigmoid(x):\n",
    "    # input x is already sigmoid\n",
    "    return x * (1.0 - x)\n",
    "\n",
    "\n",
    "def loss(pred, y):\n",
    "    return np.multiply(1 / y.shape[1],\n",
    "                       np.sum(np.multiply(.5, np.sum(np.power(np.subtract(pred, y), 2), axis=0)), axis=0))\n",
    "\n",
    "\n",
    "def dloss(pred, y):\n",
    "    return np.multiply(1 / y.shape[1], np.subtract(pred, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNet(object):\n",
    "\n",
    "    def __init__(self, batch_size, input_dim=3, hidden_dim=4, output_dim=2):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # size of layers\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim_1 = hidden_dim\n",
    "        self.hidden_dim_2 = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # activations\n",
    "        # just take ones as they are overwritten anyway\n",
    "        self.ai = np.ones((self.input_dim, self.batch_size))\n",
    "        self.ah1 = np.ones((self.hidden_dim_1, self.batch_size))\n",
    "        self.ah2 = np.ones((self.hidden_dim_2, self.batch_size))\n",
    "        self.ao = np.ones((self.output_dim, self.batch_size))\n",
    "\n",
    "        parameters, self.x, self.y = initialize(input_dim, hidden_dim, output_dim, batch_size)\n",
    "\n",
    "        self.W1, self.b1, self.W2, self.b2, self.W3, self.b3 = parameters\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        # input activations\n",
    "        self.ai = x\n",
    "\n",
    "        # hidden_1 activations\n",
    "        self.ah1 = sigmoid(np.add(np.dot(self.W1, self.ai), self.b1[:, np.newaxis]))\n",
    "\n",
    "        # hidden_2 activations\n",
    "        self.ah2 = sigmoid(np.add(np.dot(self.W2, self.ah1), self.b2[:, np.newaxis]))\n",
    "\n",
    "        # output activations\n",
    "        self.ao = np.add(np.dot(self.W3, self.ah2), self.b3[:, np.newaxis])\n",
    "\n",
    "        # return outputs for display\n",
    "        return np.concatenate((self.ai, self.ah1, self.ah2, self.ao))\n",
    "\n",
    "    def backward_pass(self):\n",
    "\n",
    "        out_error = loss(self.ao, self.y)\n",
    "\n",
    "        print(\"Error: {}\".format(out_error))\n",
    "\n",
    "        # derivative of output function f(x) = x is f'(x) = 1\n",
    "        out_delta = dloss(self.ao, self.y) * 1\n",
    "\n",
    "        # calculate error for hidden_2\n",
    "        hidden_2_error = np.dot(self.W3.T, out_delta)\n",
    "        hidden_2_delta = np.multiply(hidden_2_error, dsigmoid(self.ah2))\n",
    "\n",
    "        # calculate error for hidden_1\n",
    "        hidden_1_error = np.dot(self.W2.T, hidden_2_delta)\n",
    "        hidden_1_delta = np.multiply(hidden_1_error, dsigmoid(self.ah1))\n",
    "\n",
    "        w1_deriv = np.dot(hidden_1_delta, self.ai.T)\n",
    "        b1_deriv = np.sum(hidden_1_delta, axis=1)\n",
    "\n",
    "        w2_deriv = np.dot(hidden_2_delta, self.ah1.T)\n",
    "        b2_deriv = np.sum(hidden_2_delta, axis=1)\n",
    "\n",
    "        w3_deriv = np.dot(out_delta, self.ah2.T)\n",
    "        b3_deriv = np.sum(out_delta, axis=1)\n",
    "\n",
    "        return [w1_deriv, b1_deriv, w2_deriv, b2_deriv, w3_deriv, b3_deriv]\n",
    "\n",
    "    def check_gradients(self, eps=1e-4):\n",
    "\n",
    "        # iterate through the network once to get activations\n",
    "        output = self.forward_pass(self.x)\n",
    "\n",
    "        # backprop for comparison\n",
    "        gradients = self.backward_pass()\n",
    "\n",
    "        dw1, db1, dw2, db2, dw3, db3 = gradients\n",
    "        grad = np.concatenate((dw1.ravel(), db1.ravel(), dw2.ravel(), db2.ravel(),\n",
    "                               dw3.ravel(), db3.ravel()))\n",
    "\n",
    "        self.visualize(output, gradients)\n",
    "\n",
    "        # inital weights as 1D theta array\n",
    "        params = np.concatenate((self.W1.ravel(), self.b1.ravel(), self.W2.ravel(),\n",
    "                                 self.b2.ravel(), self.W3.ravel(), self.b3.ravel()))\n",
    "\n",
    "        # container\n",
    "        j_plus = np.zeros((params.shape[0],))\n",
    "        j_minus = np.zeros((params.shape[0],))\n",
    "        grad_numeric = np.zeros((params.shape[0],))\n",
    "\n",
    "        # Compute numeric grad\n",
    "        for i in range(len(params)):\n",
    "            theta_plus = np.copy(params)\n",
    "            theta_plus[i] += eps\n",
    "            w_plus = self.array_to_weights(theta_plus)\n",
    "            j_plus[i] = loss(self.predict(self.x, w_plus), self.y)\n",
    "\n",
    "            theta_minus = np.copy(params)\n",
    "            theta_minus[i] -= eps\n",
    "            w_minus = self.array_to_weights(theta_minus)\n",
    "            j_minus[i] = loss(self.predict(self.x, w_minus), self.y)\n",
    "\n",
    "            # Compute approx. gradient\n",
    "            grad_numeric[i] = np.subtract(j_plus[i], j_minus[i]) / (2 * eps)\n",
    "\n",
    "        # compute relative error as propsed in cs231n\n",
    "        difference = np.divide(np.linalg.norm(np.subtract(grad, grad_numeric)),\n",
    "                               max(np.linalg.norm(grad), np.linalg.norm(grad_numeric)))\n",
    "\n",
    "        if difference < 1e-7:\n",
    "            print(\"\\033[92m\" + \"Backprop works perfectly! Difference = \" + str(\n",
    "                difference) + \"\\033[0m\")\n",
    "\n",
    "        elif difference < 1e-4:\n",
    "            print(\"\\033[93m\" + \"Backprop is usually okay! Difference = \" + str(\n",
    "                difference) + \"\\033[0m\")\n",
    "        else:\n",
    "            print(\"\\033[91m\" + \"Error in backprop (or gradient check)! Difference = \" + str(\n",
    "                difference) + \"\\033[0m\")\n",
    "\n",
    "        return difference\n",
    "\n",
    "    def predict(self, x, weights):\n",
    "        self.W1, self.b1, self.W2, self.b2, self.W3, self.b3 = weights\n",
    "        return self.forward_pass(x)[-self.output_dim:]\n",
    "\n",
    "    def array_to_weights(self, array):\n",
    "        w1_idx = self.W1.ravel().shape[0]\n",
    "        b1_idx = w1_idx + self.b1.ravel().shape[0]\n",
    "        w2_idx = b1_idx + self.W2.ravel().shape[0]\n",
    "        b2_idx = w2_idx + self.b2.ravel().shape[0]\n",
    "        w3_idx = b2_idx + self.W3.ravel().shape[0]\n",
    "        b3_idx = w3_idx + self.b3.ravel().shape[0]\n",
    "\n",
    "        W1 = array[0:w1_idx].reshape(self.W1.shape)\n",
    "        b1 = array[w1_idx:b1_idx].reshape(self.b1.shape)\n",
    "        W2 = array[b1_idx:w2_idx].reshape(self.W2.shape)\n",
    "        b2 = array[w2_idx:b2_idx].reshape(self.b2.shape)\n",
    "        W3 = array[b2_idx:w3_idx].reshape(self.W3.shape)\n",
    "        b3 = array[w3_idx:b3_idx].reshape(self.b3.shape)\n",
    "\n",
    "        return [W1, b1, W2, b2, W3, b3]\n",
    "\n",
    "    @staticmethod\n",
    "    def visualize(activations, gradients):\n",
    "        \n",
    "        df = pd.DataFrame(activations).T\n",
    "        df.columns = [\"x1\", \"x2\", \"x3\", \"h11\", \"h12\", \"h13\", \"h14\", \"h21\", \"h22\", \"h23\", \"h24\", \"o1\", \"o2\"]\n",
    "        # round by 3 decimals to shown in one line\n",
    "        print(tabulate(df, headers='keys', tablefmt='psql', floatfmt=\".3f\"))\n",
    "        print(\"\\n\")\n",
    "\n",
    "        dw1, db1, dw2, db2, dw3, db3 = gradients\n",
    "\n",
    "        print(\"dL/dW1:\\n {}\\n\".format(dw1))\n",
    "        print(\"dL/db1:\\n {}\\n\".format(db1))\n",
    "        print(\"dL/dW2:\\n {}\\n\".format(dw2))\n",
    "        print(\"dL/db2:\\n {}\\n\".format(db2))\n",
    "        print(\"dL/dW3:\\n {}\\n\".format(dw3))\n",
    "        print(\"dL/db3:\\n {}\\n\".format(db3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(batch_size=5):\n",
    "    # X = np.array([[.13, .68, .80, .57, .97],\n",
    "    #               [.63, .89, .50, .35, .71],\n",
    "    #               [.50, .23, .24, .79, .50]])\n",
    "\n",
    "    nn = NeuralNet(batch_size=batch_size, input_dim=3, hidden_dim=4, output_dim=2)\n",
    "    nn.check_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.9992378792255207\n",
      "+----+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+-------+\n",
      "|    |    x1 |    x2 |    x3 |   h11 |   h12 |   h13 |   h14 |   h21 |   h22 |   h23 |   h24 |     o1 |    o2 |\n",
      "|----+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+-------|\n",
      "|  0 | 0.472 | 0.826 | 0.420 | 0.501 | 0.497 | 0.499 | 0.499 | 0.498 | 0.502 | 0.501 | 0.498 | -0.020 | 0.011 |\n",
      "|  1 | 0.721 | 0.190 | 0.768 | 0.504 | 0.500 | 0.500 | 0.498 | 0.498 | 0.502 | 0.501 | 0.498 | -0.020 | 0.011 |\n",
      "|  2 | 0.189 | 0.020 | 0.963 | 0.504 | 0.502 | 0.501 | 0.500 | 0.498 | 0.502 | 0.501 | 0.498 | -0.020 | 0.011 |\n",
      "|  3 | 0.435 | 0.703 | 0.893 | 0.503 | 0.499 | 0.500 | 0.500 | 0.498 | 0.502 | 0.501 | 0.498 | -0.020 | 0.011 |\n",
      "|  4 | 0.823 | 0.347 | 0.135 | 0.501 | 0.497 | 0.498 | 0.498 | 0.498 | 0.502 | 0.501 | 0.498 | -0.020 | 0.011 |\n",
      "+----+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+-------+\n",
      "\n",
      "\n",
      "dL/dW1:\n",
      " [[-2.38730851e-06 -1.75156641e-06 -6.89294115e-06]\n",
      " [ 5.16880980e-06  3.73861976e-06  1.40716571e-05]\n",
      " [-3.91917886e-06 -2.82212506e-06 -1.04682966e-05]\n",
      " [ 1.75889979e-07  4.17668990e-07  5.10164484e-06]]\n",
      "\n",
      "dL/db1:\n",
      " [-7.06209929e-06  1.47647546e-05 -1.10710651e-05  3.35239146e-06]\n",
      "\n",
      "dL/dW2:\n",
      " [[0.00145273 0.00144393 0.00144232 0.00144015]\n",
      " [0.00154007 0.00153071 0.00152922 0.00152696]\n",
      " [0.00054392 0.00054074 0.00053901 0.00053802]\n",
      " [0.00020042 0.00019913 0.00019962 0.00019944]]\n",
      "\n",
      "dL/db2:\n",
      " [0.00288082 0.003055   0.00107337 0.00040077]\n",
      "\n",
      "dL/dW3:\n",
      " [[-0.25856098 -0.26044299 -0.25997866 -0.25872448]\n",
      " [ 0.20803913  0.20955868  0.20918306  0.20817165]]\n",
      "\n",
      "dL/db3:\n",
      " [-0.51910648  0.41767941]\n",
      "\n",
      "\u001b[92mBackprop works perfectly! Difference = 6.672183200217608e-12\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
